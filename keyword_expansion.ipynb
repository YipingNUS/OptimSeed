{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Keyword Mining\n",
    "\n",
    "Mine candidate keywords associated with the category name. The keywords are used as (noisy) candidate seed words for dataless classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from math import log\n",
    "from tqdm import tnrange\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import math\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = English().Defaults.create_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" available config files:\n",
    "    20NG-baseball-hockey.yaml 20NG-space-med.yaml 20NG-ibm-mac.yaml   \n",
    "    AGNews-world-tech.yaml AGNews-business-sports.yaml \n",
    "    NYT-football-soccer.yaml NYT-movies-television.yaml NYT-international_business-economy.yaml\n",
    "    Yelp-pos-neg.yaml\n",
    "    IMDB-pos-neg.yaml\n",
    "\"\"\"\n",
    "config_file_folder = 'configs/'\n",
    "config_file = 'IMDB-pos-neg.yaml'\n",
    "\n",
    "with open(config_file_folder + config_file) as f:\n",
    "    config = yaml.load(f, Loader=yaml.Loader)\n",
    "    \n",
    "categories = config['categories']\n",
    "seed_words = config['seed_words']\n",
    "output_file = config['kw_file']\n",
    "corpus_path = config['train_corpus_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "# the acceptable POS tags, may vary based on the task (topic/sentiment classification)\n",
    "ACCEPTABLE_TAGS_TOPIC = set(['FW', 'NN', 'NNS', 'NNP', 'NNPS']) \n",
    "ACCEPTABLE_TAGS_SENTIMENT = set(['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS'])\n",
    "ACCEPTABLE_TAGS_SENTIMENT = set(['JJ', 'JJR', 'JJS'])\n",
    "def is_valid_topic(word):\n",
    "    tag = pos_tag([word])[0][1]  # pos_tag returns [('cat', 'NN')]\n",
    "    if tag in ACCEPTABLE_TAGS_TOPIC:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_valid_sentiment(word):\n",
    "    tag = pos_tag([word])[0][1]  # pos_tag returns [('cat', 'NN')]\n",
    "    if tag in ACCEPTABLE_TAGS_SENTIMENT:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the frequency threshold. Words with lower frequency will be filtered out\n",
    "FREQ_THRESHOLD = 3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, lower_case=True):\n",
    "    if lower_case:\n",
    "        text = text.lower()\n",
    "    tokens = tokenizer(text)\n",
    "    return [token.text for token in tokens if token.text.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Read the corpus\n",
    "\n",
    "Please run only one subsection based on the dataset you're using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 1.1 Load 20NG dataset\n",
    "\n",
    "**TODO:** uncomment the list of categories and seed words to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# note that dataset.target_names may not be in the same order as categories\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories= categories, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_train = newsgroups_train.data\n",
    "target_train = newsgroups_train.target\n",
    "target_names = newsgroups_train.target_names\n",
    "num_train_docs = len(data_train)\n",
    "print(\"[INFO] Total\", num_train_docs, \"documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 1.2 Load AG news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "corpus_path = '/Users/admin/corpora/TextClassification/AG News Corpus/'\n",
    "all_classes = ['World', 'Sports', 'Business', 'Tech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(corpus_path+\"train.csv\", names=['label', 'title', 'description'])\n",
    "\n",
    "# replace the label index with the label\n",
    "df['label'] = df['label'].apply(lambda x: all_classes[int(x)-1])\n",
    "df['text'] = df.apply(lambda x: x['title'].lower() + ' ' + x['description'].lower(), axis=1)\n",
    "print(len(df))\n",
    "df = df[df['label'].isin(categories)]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_train = df['text'].tolist()\n",
    "num_train_docs = len(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 1.3 Load IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "corpus_path = '/Users/admin/corpora/SentimentAnalysis/aclImdb/train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset = load_files(corpus_path)\n",
    "data_train = dataset.data\n",
    "num_train_docs = len(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 1.4 Load Yelp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "corpus_path = '/Users/admin/corpora/SentimentAnalysis/yelp/train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset = load_files(corpus_path)\n",
    "data_train = dataset.data\n",
    "num_train_docs = len(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 1.5 Load NYT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset = load_files(corpus_path, categories=categories)\n",
    "data_train = dataset.data\n",
    "num_train_docs = len(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e8b3a7ba9684aca91f0a7cd8aeeb83d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=75000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "counter = Counter()\n",
    "for i in tnrange(num_train_docs):\n",
    "    tokens = tokenize(str(data_train[i]))\n",
    "    counter.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Vocab size: 58215\n"
     ]
    }
   ],
   "source": [
    "vocab = { x: count for x, count in counter.items() if count >= FREQ_THRESHOLD}\n",
    "print(\"[INFO] Vocab size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01d872fb70044929dcdf1b1fcaf6436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=75000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inverted_index = defaultdict(set)\n",
    "for i in tnrange(num_train_docs):\n",
    "    unique_tokens = set(tokenize(str(data_train[i])))\n",
    "    [inverted_index[tok].add(i) for tok in unique_tokens if tok in vocab]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Use PMI to rank the keywords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great 18666\n",
      "worst 6521\n"
     ]
    }
   ],
   "source": [
    "# sanity check. make sure the seed words are present\n",
    "for s in seed_words:\n",
    "    print(s, len(inverted_index[s]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seedword: great\n",
      "['great', 'deal', 'greatest', 'fantastic', 'fenn', 'awesome', 'paco', 'yokai', 'greatness', 'shakes', 'doubtlessly', 'sherilyn', 'composers', 'corbucci', 'job', 'wonderful']\n",
      "['great', 'greatest', 'fantastic', 'unkillable', 'underrated', 'rumble', 'influential', 'gorgeous', 'classic', 'fabulous', 'nyree', 'solid', 'scorsese', 'memorable', 'meticulous', 'outstanding']\n",
      "Seedword: worst\n",
      "['worst', 'ever', 'manos', 'awful', 'terrible', 'horrible', 'medved', 'seen', 'misfortune', 'worse', 'crap', 'garbage', 'badness', 'atrocious', 'displeasure', 'boll']\n",
      "['worst', 'terrible', 'horrible', 'worse', 'atrocious', 'bad', 'unfunny', 'stupid', 'laughable', 'pathetic', 'horrendous', 'unintentional', 'ridiculous', 'crappy', 'redeemable', 'entire']\n"
     ]
    }
   ],
   "source": [
    "# PMI\n",
    "MIN_COOCCURENCE = 3\n",
    "output_keywords = list()\n",
    "for s in seed_words:\n",
    "    seed_docs = inverted_index[s]\n",
    "    result = dict()\n",
    "    for w, docs in inverted_index.items():\n",
    "        #if w == s:  # skip the seed word itself\n",
    "        #    continue\n",
    "        cand_docs = inverted_index[w]\n",
    "        intersection = len(seed_docs.intersection(cand_docs))\n",
    "        if intersection < MIN_COOCCURENCE:\n",
    "            continue\n",
    "        pmi = log(intersection*num_train_docs/(len(cand_docs)*len(seed_docs)))\n",
    "        pmi_freq = pmi * log(intersection)\n",
    "        result[w] = pmi_freq\n",
    "    top_keywords = sorted(result, key=result.get, reverse=True)[:200]\n",
    "    print(\"Seedword:\", s)\n",
    "    print(top_keywords[:16])\n",
    "    if output_file.startswith('20NG') or output_file.startswith('AGNews') or output_file.startswith(\"NYT\"):\n",
    "        filtered_top_keywords = [kw for kw in top_keywords if is_valid_topic(kw)]\n",
    "    elif output_file.startswith('IMDB') or output_file.startswith('Yelp'):\n",
    "        filtered_top_keywords = [kw for kw in top_keywords if is_valid_sentiment(kw)]\n",
    "    else:\n",
    "        print(\"Dataset unknown:\", output_file)\n",
    "    print(filtered_top_keywords[:16])\n",
    "    output_keywords.append(filtered_top_keywords[:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# MMR\n",
    "MIN_COOCCURENCE = 3\n",
    "output_keywords = list()\n",
    "result = dict()\n",
    "\n",
    "for w, docs in inverted_index.items():\n",
    "    cand_docs = inverted_index[w]\n",
    "    scores = list()\n",
    "    if output_file.startswith('20NG') or output_file.startswith('AGNews') or output_file.startswith(\"NYT\"):\n",
    "        if not is_valid_topic(w):\n",
    "            continue\n",
    "    elif output_file.startswith('IMDB') or output_file.startswith('Yelp'):\n",
    "        if not is_valid_sentiment(w):\n",
    "            continue\n",
    "    else:\n",
    "        print(\"Dataset unknown:\", output_file)\n",
    "        continue\n",
    "        \n",
    "    for s in seed_words:\n",
    "        seed_docs = inverted_index[s]\n",
    "        #if w == s:  # skip the seed word itself\n",
    "        #    continue\n",
    "        intersection = len(seed_docs.intersection(cand_docs))\n",
    "        if intersection < MIN_COOCCURENCE:\n",
    "            pmi_freq == 0\n",
    "        else: \n",
    "            pmi = log(intersection*num_train_docs/(len(cand_docs)*len(seed_docs)))\n",
    "            pmi = log(len(cand_docs)) * pmi\n",
    "        scores.append(pmi)\n",
    "    result[w] = scores  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('great', [13.677612372086209, -3.7359466932543532]), ('greatest', [3.8321081557267003, -2.2257754820635496]), ('fantastic', [3.814703435769346, -4.82743346873582]), ('underrated', [3.1029038301076493, -5.260466778555113]), ('influential', [3.065025737381599, -2.851025732579256]), ('gorgeous', [3.0007295066517226, -2.576981045893402]), ('fabulous', [2.9568074446922963, -4.469156114835475]), ('classic', [2.8889776195575174, -1.7343578428969344]), ('meticulous', [2.856367263879169, -2.2652857887620335]), ('solid', [2.7903928822935664, -3.7371440823140123]), ('ian', [2.7887092955681654, -0.7084548605945855]), ('marvelous', [2.7707050643325997, -8.183672703791448]), ('outstanding', [2.7552408962843566, -4.940259611889717]), ('memorable', [2.7521923136617965, -2.2101482148649056]), ('ensemble', [2.6649206306128796, -4.085265448866313]), ('best', [2.641498169595701, -0.057255340055329104])]\n",
      "[('worst', [-3.336432566795995, 21.451599503305832]), ('terrible', [-1.5736557516764749, 7.699149490838816]), ('horrible', [-0.9833892709519753, 7.662968553920748]), ('worse', [-1.6408437735786567, 7.232374915329133]), ('atrocious', [-3.567711567651832, 7.033582083300243]), ('bad', [-0.7498471425284862, 6.460242055727636]), ('unfunny', [-1.7420251726359328, 6.4343395500234575]), ('stupid', [-1.2060982768232986, 6.2536471496293435]), ('laughable', [-1.987663881008111, 5.987145313258553]), ('pathetic', [-1.4753035249987583, 5.958258104519805]), ('horrendous', [-1.4319186433644353, 5.890508851515628]), ('unintentional', [-0.832183992835248, 5.722376117665624]), ('ridiculous', [-0.9592228794976617, 5.2663981033549385]), ('redeemable', [-1.0189238097519826, 4.993603064428117]), ('crappy', [-0.8191781074939299, 4.9665449933014285]), ('lowest', [-1.0695858398216533, 4.540983107751112])]\n"
     ]
    }
   ],
   "source": [
    "for i, s in enumerate(seed_words):\n",
    "    top_keywords = sorted(result.items(), key=lambda kv: kv[1][i]-max(kv[1][1-i], 0), reverse=True)\n",
    "    print(top_keywords[:16])\n",
    "    output_keywords.append([k for k, _ in top_keywords[:16]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['great', 'greatest', 'fantastic', 'underrated', 'influential', 'gorgeous', 'fabulous', 'classic', 'meticulous', 'solid', 'ian', 'marvelous', 'outstanding', 'memorable', 'ensemble', 'best'], ['worst', 'terrible', 'horrible', 'worse', 'atrocious', 'bad', 'unfunny', 'stupid', 'laughable', 'pathetic', 'horrendous', 'unintentional', 'ridiculous', 'redeemable', 'crappy', 'lowest']]\n"
     ]
    }
   ],
   "source": [
    "print(output_keywords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
